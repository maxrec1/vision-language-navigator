# Vision Language Navigator

ROS 2 + TurtleBot4 + Phi-3 LLM-based navigation system for converting natural language commands to structured robot navigation goals.

## ğŸš€ Quick Start

### Prerequisites
- **ROS 2 Jazzy** installed
- **Ollama** (local LLM runner)
- **TurtleBot4 Gazebo simulator** packages
- Python 3.10+

### Installation

1. **Clone repository**
   ```bash
   git clone https://github.com/maxrec1/vision-language-navigator.git
   cd vision-language-navigator
   ```

2. **Install Ollama + Phi-3**
   ```bash
   # See docs/PHASE1_SETUP.md for detailed instructions
   curl -fsSL https://ollama.ai/install.sh | sh
   ollama pull phi3
   ```

3. **Install Python dependencies**
   ```bash
   pip install requests --break-system-packages
   ```

    **Test Command parser**
     ```bash
   python3 src/tb4_gz_rqt_launch/ollama_test.py   # might take some time to run
    ```

4. **Build ROS 2 workspace**
   ```bash
   colcon build
   source install/setup.bash
   ```

5. **Run simulator + command parser**
   ```bash
   # Launch TurtleBot4 simulator + command parser
   ros2 launch tb4_gz_rqt_launch tb4_gz_rqt_launch.launch.py
   ```

## ğŸ“‹ Project Structure

```
vision-language-navigator/
â”œâ”€â”€ README.md                          â† You are here
â”œâ”€â”€ .gitignore                         â† Git ignore rules
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PHASE1_SETUP.md               â† Installation & setup guide
â”œâ”€â”€ src/tb4_gz_rqt_launch/
â”‚   â”œâ”€â”€ README_COMMAND_PARSER.md       â† Command parser API reference
â”‚   â”œâ”€â”€ ollama_test.py                 â† Test script (standalone)
â”‚   â”œâ”€â”€ command_parser_node.py         â† ROS 2 service node (Phase 2)
â”‚   â”œâ”€â”€ package.xml
â”‚   â”œâ”€â”€ setup.py
â”‚   â””â”€â”€ tb4_gz_rqt_launch/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ... (other modules)
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ tb4_gz_rqt_launch.launch.py    â† Main launch file
â”‚   â””â”€â”€ command_parser.launch.py       â† Parser node launcher (Phase 2)
â””â”€â”€ README.md (this file)
```

## ğŸ§  How It Works

### Phase 1: LLM Command Parser âœ…
Extracts structured navigation goals from natural language using **Phi-3** running locally via **Ollama**.

**Input**: `"go to the chair"`  
**Output**: `{"target": "chair", "relation": null, "reference": null}`

**Features:**
- Runs locally (privacy-preserving)
- GPU-accelerated inference
- Robust JSON extraction (handles markdown wrapping)
- Few-shot prompting for reliable output

### Phase 2: ROS 2 Service Integration (In Progress)
Wraps Phase 1 in a ROS 2 service node for TurtleBot4 integration.

**Service**: `/parse_command` (ParseCommand.srv)  
**Interface**: Converts natural language â†’ structured navigation goals

### Phase 3: Navigation Stack (Planned)
Integrates with TurtleBot4 navigation stack and YOLO object detection.

## ğŸ“– Documentation

- **[Phase 1 Setup Guide](docs/PHASE1_SETUP.md)** â€” How to install Ollama + Phi-3 + test the parser
- **[Command Parser Module](src/tb4_gz_rqt_launch/README_COMMAND_PARSER.md)** â€” API reference & usage examples

## ğŸ§ª Testing

### Run Phase 1 Test (Standalone)
```bash
python3 src/tb4_gz_rqt_launch/ollama_test.py
```

**Output:**
```
============================================================
OLLAMA + PHI-3 COMMAND PARSER TEST
============================================================

[1] Input: 'Go to the chair'
    Output: {
      "target": "chair",
      "relation": null,
      "reference": null
    }

[2] Input: 'Find the table near the window'
    Output: {
      "target": "table",
      "relation": "near",
      "reference": "window"
    }

...

[6] INTERACTIVE MODE - Enter your own command
ğŸ¤– Enter a navigation command (or press Enter to skip):
```

### Check Ollama Status
```bash
curl http://localhost:11434/api/tags
ollama list
```

## ğŸ”§ Configuration

### Ollama Settings
- **Model**: Phi-3 (3.8B parameters, 2.2 GB)
- **API Endpoint**: `http://localhost:11434/api/generate`
- **Inference Timeout**: 120 seconds

### System Prompt
The parser uses a few-shot prompting strategy to force strict JSON output. See [docs/PHASE1_SETUP.md](docs/PHASE1_SETUP.md#system-prompt-strategy) for details.

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Model | Phi-3 (3.8B) |
| Inference Time | ~60-80s per command (GPU) |
| Accuracy | High for navigation commands |
| JSON Parsing Success | >95% |
| Latency Type | Offline planning (not real-time) |

## âš ï¸ Troubleshooting

### Ollama connection failed
```bash
# Make sure Ollama service is running
sudo systemctl start ollama
# or
ollama serve
```

### Import error: `requests` not found
```bash
pip install requests --break-system-packages
```

### Slow inference
- Check GPU availability: `nvidia-smi`
- Consider lighter model (Llama 2 7B) if needed
- Monitor memory usage: `watch nvidia-smi`

### JSON parsing errors
- Re-run the test (LLMs are non-deterministic)
- Check Ollama logs: `journalctl -u ollama -f`
- See [docs/PHASE1_SETUP.md#troubleshooting](docs/PHASE1_SETUP.md#troubleshooting)

## ğŸ“ Project Phases

### âœ… Phase 1: LLM Command Parser
- Install Ollama + Phi-3 âœ…
- Implement JSON extraction âœ…
- Test parser locally âœ…
- Document API âœ…

### ğŸ”„ Phase 2: ROS 2 Service Integration
- Create `.srv` interface (ParseCommand)
- Implement service node
- Launch file integration
- ROS CLI testing

### ğŸ“… Phase 3: Navigation Stack
- Integrate with TurtleBot4 move_base
- Add YOLO object detection
- Handle multi-step navigation
- Real-time visualization

## ğŸ¤ Contributing

1. Fork the repo
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see LICENSE file for details.

## ğŸ‘¤ Author

**maxrec** â€” ROS 2 + Vision Language Navigation Research

## ğŸ™ Acknowledgments

- Phi-3 by Microsoft
- Ollama for local LLM inference
- TurtleBot4 by Clearpath Robotics
- ROS 2 Community

---

**Last Updated**: January 23, 2026  
**Status**: Phase 1 Complete, Phase 2 In Progress
